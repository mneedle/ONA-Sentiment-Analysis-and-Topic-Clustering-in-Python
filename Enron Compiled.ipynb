{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Programming Team Project by Max Needle, Sammie Kim, and Toby Du\n",
    "\n",
    "This project focuses on the analysis of the Enron Emails corpus. We approached it by using sentiment analysis, organizational network analysis (ONA), topic clustering, and a number of visualizations. In this project, we first extracted data from each email (i.e., date, message, recipient, sender) and incorporated additional information about the job level of the email senders (\"Titles.xlsx\") and Enron’s monthly stock price (\"Enron_Monthly.xlsx\"). We then conducted sentiment analysis and topic clustering (5 topics) on the messages. We used ONA and sentiment analysis to investigate differences in network centrality (degree and betweenness centrality) and email sentiment between emails sent by management and those sent by other employees. We then used email topic probability (for each topic), email sentiment, and Enron’s stock price to investigate correlations between these variables over time. Finally, we used scatterplots, word clouds, network diagrams and time series visualization to make inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "from email.parser import Parser\n",
    "from afinn import Afinn\n",
    "from wordcloud import WordCloud\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import re\n",
    "import pingouin as pg\n",
    "import researchpy as rp\n",
    "import plotly.express as px\n",
    "from textblob import TextBlob\n",
    "import statistics\n",
    "import wget\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.graphics.factorplots import interaction_plot\n",
    "import statsmodels.stats.multicomp\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct full df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Series of all full emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: path to directory\n",
    "# Processing: iterates through path to get to every file\n",
    "# Output: list of filepaths in a directory\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath) # recursion \n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading and unzipping the Enron Emails corpus takes several minutes\n",
    "wget.download('https://www.cs.cmu.edu/~enron/enron_mail_20150507.tar.gz','enron_mail_20150507.tar.gz')\n",
    "!tar xzf enron_mail_20150507.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all filepaths in main directory\n",
    "files= getListOfFiles('maildir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with all of the full emails\n",
    "emails= [open(f, mode='r', encoding=\"utf8\", errors='ignore').read() for f in files]\n",
    "df= pd.Series(emails)\n",
    "df= df.to_frame('Email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the email entries to Parser datatypes to easily extract info on sender, recipients, subject, message, date\n",
    "df['Email']= df['Email'].apply(Parser().parsestr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract message info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column for the subject and message of the email (to be used later for topic modeling and wordclouds)\n",
    "df['Message']= [e['subject']+\" \"+e.get_payload().replace('\\n',' ') for e in df['Email']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column for sentiment of message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label sentiment analyzer\n",
    "af = Afinn()\n",
    "# analyze sentiment (this also takes a while to run)\n",
    "df['Sentiment']= df['Message'].apply(af.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns for the probabilities of the top 5 topics in each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "\n",
    "# create list of 150 most frequently-occuring words\n",
    "words= (\" \".join(df['Message'])).lower()\n",
    "counter = Counter(words.split())\n",
    "common = counter.most_common()\n",
    "tk= list(pd.DataFrame(common[0:150])[0])\n",
    "\n",
    "# remove frequently-occuring words that are stopwords (will be removed by CountVectorizer)\n",
    "non_stopword = [t for t in tk if t not in stopwords.words('english')]\n",
    "#print(non_stopword) # review common words\n",
    "to_remove = ['to','please','subject','pm','would','cc','re','may','from','said','get','know','one','need','forwarded',\n",
    "             'sent','could','image','think','also','information','message','original','like','let','us','last',\n",
    "             'attached','meeting','day','make','two','email','first','corp','want','thanks','see','next','mark',\n",
    "             'use','contact','take']\n",
    "\n",
    "# drop these common words\n",
    "def drop_email_words(message):\n",
    "    dropped = []\n",
    "    for word in message.split():\n",
    "        if word not in to_remove:\n",
    "            dropped += [word]\n",
    "    return \" \".join(dropped)\n",
    "\n",
    "# create new preprocessed column with the lowercased message and without punctuation, numbers, new line, tab, and extra white spaces\n",
    "df['Preprocess']=df['Message'].str.replace(r'[^\\w\\s]','')\n",
    "df['Preprocess']=df['Preprocess'].str.replace('\\d+', '')\n",
    "df['Preprocess']=df['Preprocess'].str.replace(\"\\n\",\"\")\n",
    "df['Preprocess']=df['Preprocess'].str.replace(\"\\t\",\"\")\n",
    "df['Preprocess']=df['Preprocess'].str.replace(' +',' ')\n",
    "df['Preprocess']=df['Preprocess'].str.lower()\n",
    "df['Preprocess']=df['Preprocess'].apply(drop_email_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document-term matrix for preprocessed messages (documents)\n",
    "count_vect = CountVectorizer(ngram_range= (1,2), max_df=0.6, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(df['Preprocess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 5 topic clusters\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42) # 5 topics\n",
    "LDA.fit(doc_term_matrix)\n",
    "topic_values = LDA.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 15 words with highest probabilities for each of the 5 topics\n",
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(f'Top 15 words for topic #{i+1}: '+\", \".join([count_vect.get_feature_names()[i] for i in topic.argsort()[-15:]]))\n",
    "    print('')\n",
    "\n",
    "# based on top 15 words, these topics are interpreted as:\n",
    "# 1. Reporting\n",
    "# 2. Revenue\n",
    "# 3. Regulation\n",
    "# 4. Management\n",
    "# 5. Energy Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds columns with the probabilities of each topic in each message   \n",
    "topics_df= pd.DataFrame(topic_values, columns= [\"Topic1\",\"Topic2\", \"Topic3\",\"Topic4\", \"Topic5\"])\n",
    "df= pd.concat([df,topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Recipient info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new column for the list of recipients of each email and delete all email entries where there are no recipients\n",
    "df['Recipient_list']= [e['To'] for e in df['Email']]\n",
    "df= df[df['Recipient_list'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean recipient data and transform it into lists of string email addresses\n",
    "df['Recipient_list']= [r.replace(\"\\n\\t\", \"\").split(', ') for r in df['Recipient_list']];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Series of the recipient lists expanded so that, for each email, each recipient has its own row but \n",
    "# they all have the index of the original email\n",
    "recipient= (df.Recipient_list.apply(pd.Series)\n",
    "              .stack()\n",
    "              .reset_index(level=1, drop=True)\n",
    "              .to_frame('Recipient'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the expanded recipient list onto the original DataFrame on the indexes (this will copy the email\n",
    "# to all recipient rows) and drop the old column of recipient lists\n",
    "df= df.join(recipient)\n",
    "df.drop('Recipient_list',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Sender info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column for the senders of every email and delete all email entries where there is no sender\n",
    "df['Sender']= [e['from'] for e in df['Email']]\n",
    "df= df[df['Sender'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract date info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column for the date of the email\n",
    "df['Date']= [datetime.strptime(\"/\".join(e['date'].split()[1:4]), '%d/%b/%Y') for e in df['Email']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index so that each row has its own unique index\n",
    "df= df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column with full message (no longer valueable)\n",
    "df= df.drop('Email', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns for sender and recipient job level groups (only available for a subset of emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job Level groups (annotated in \"Titles.xlsx\")\n",
    "\n",
    "\n",
    "Group A: management\n",
    "- CEO\n",
    "- COO\n",
    "- Director\n",
    "- General Counsel\n",
    "- Managing Director\n",
    "- President\n",
    "- Vice President\n",
    "\n",
    "Group B: other\n",
    "- Administrative Assistant\n",
    "- Analyst\n",
    "- Government Relations Executive\n",
    "- In-House Lawyer\n",
    "- Manager\n",
    "- Senior Analyst\n",
    "- Senior Specialist\n",
    "- Specialist\n",
    "- Trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in titles and groups to df\n",
    "url_titles = 'https://raw.github.com/mneedle/ONA-Sentiment-Analysis-and-Topic-Clustering-in-Python/master/Titles.xlsx'\n",
    "titles_groups = pd.read_excel(url_titles)\n",
    "\n",
    "title_subset = titles_groups[[\"Email\",\"Title\"]]\n",
    "titles = dict(zip(title_subset.Email, title_subset.Title))\n",
    "df[\"SenderTitle\"]= df[\"Sender\"].map(titles)\n",
    "df[\"RecipientTitle\"]= df[\"Recipient\"].map(titles)\n",
    "\n",
    "group_subset= titles_groups[[\"Email\",\"Group\"]]\n",
    "groups= dict(zip(group_subset.Email, group_subset.Group))\n",
    "df[\"SenderGroup\"]= df[\"Sender\"].map(groups)\n",
    "df[\"RecipientGroup\"]= df[\"Recipient\"].map(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column for monthly stock price for month of email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in stock prices to df\n",
    "url_stock = 'https://raw.github.com/mneedle/ONA-Sentiment-Analysis-and-Topic-Clustering-in-Python/master/Enron_Monthly.xlsx'\n",
    "stock = pd.read_excel(url_stock)\n",
    "prices= stock[[\"Date\",\"Last Price\"]]\n",
    "prices['Month_Year_String']= [d.strftime('%Y-%m') for d in prices['Date']]\n",
    "monthly_prices= dict(zip(prices['Month_Year_String'], prices['Last Price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column with years and months, map monthly stock prices, and delete column\n",
    "df['Date_String']= [d.strftime('%Y-%m') for d in df['Date']]\n",
    "df['Monthly Stock Price']= df['Date_String'].map(monthly_prices)\n",
    "df= df.drop('Date_String',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('df_full.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel(\"df_full.xlsx\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create scatterplot of emails sent over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot email sent over time\n",
    "fig = px.scatter(df, x=\"Date\", y=\"Sender\",\n",
    "                 title=\"Sent Email Over Time\",\n",
    "                )\n",
    "fig.update_yaxes(nticks=30)\n",
    "fig.update_xaxes(nticks=50)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analye categorical sentiment and produce wordclouds for emails sent by each job level group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupA: Senior Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and clean all the words in emails sent by Senior Management\n",
    "common_words_GroupA = (\" \".join([str(message) for message in df[\"Preprocess\"][df['SenderGroup']=='GroupA']]))\n",
    "common_GroupA_no_stop= [w for w in common_words_GroupA.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing the words into positive, negative and neutral\n",
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "scores_of_words = []\n",
    "for text in common_GroupA_no_stop:\n",
    "    blob = TextBlob(text)\n",
    "    scores_of_words.append(blob.sentiment.polarity)\n",
    "    if(blob.sentiment.subjectivity>0.1):\n",
    "        if(blob.sentiment.polarity==0.0):\n",
    "            neutral.append(text)\n",
    "        if(blob.sentiment.polarity>0.0):\n",
    "            positive.append(text)\n",
    "        if(blob.sentiment.polarity<0.0):\n",
    "            negative.append(text)\n",
    "            \n",
    "# removing duplicates words from positive,negative and neutral words list.\n",
    "uniqueWords_neutral = []\n",
    "uniqueWords_positive = []\n",
    "uniqueWords_negative = []\n",
    "\n",
    "for i in positive:\n",
    "      if not i in uniqueWords_positive:\n",
    "            uniqueWords_positive.append(i);\n",
    "for j in negative:\n",
    "      if not j in uniqueWords_negative:\n",
    "            uniqueWords_negative.append(j);\n",
    "for k in neutral:\n",
    "      if not k in uniqueWords_neutral:\n",
    "            uniqueWords_neutral.append(k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar graph\n",
    "names = [\"positive\",\"negative\",\"neutral\"]\n",
    "values = [len(uniqueWords_positive),len(uniqueWords_negative),len(uniqueWords_neutral)]\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(10, 5), sharey=True)\n",
    "axs.bar(names, values)                   \n",
    "axs.set_title('Senior Management Words ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word cloud \n",
    "all_words = uniqueWords_positive + uniqueWords_negative + uniqueWords_neutral \n",
    "wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(all_words))\n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupB: Other Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and clean all the words in emails sent by other employees\n",
    "common_words_GroupB = (\" \".join([str(message) for message in df[\"Preprocess\"][df['SenderGroup']=='GroupB']]))\n",
    "common_GroupB_no_stop= [w for w in common_words_GroupB.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing the words into positive, negative and neutral\n",
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "scores_of_words = []\n",
    "for text in common_GroupB_no_stop:\n",
    "    blob = TextBlob(text)\n",
    "    scores_of_words.append(blob.sentiment.polarity)\n",
    "    if(blob.sentiment.subjectivity>0.1):\n",
    "        if(blob.sentiment.polarity==0.0):\n",
    "            neutral.append(text)\n",
    "        if(blob.sentiment.polarity>0.0):\n",
    "            positive.append(text)\n",
    "        if(blob.sentiment.polarity<0.0):\n",
    "            negative.append(text)\n",
    "            \n",
    "# removing duplicates words from positive,negative and neutral words list.\n",
    "uniqueWords_neutral = []\n",
    "uniqueWords_positive = []\n",
    "uniqueWords_negative = []\n",
    "\n",
    "for i in positive:\n",
    "      if not i in uniqueWords_positive:\n",
    "            uniqueWords_positive.append(i);\n",
    "for j in negative:\n",
    "      if not j in uniqueWords_negative:\n",
    "            uniqueWords_negative.append(j);\n",
    "for k in neutral:\n",
    "      if not k in uniqueWords_neutral:\n",
    "            uniqueWords_neutral.append(k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar graph\n",
    "names = [\"positive\",\"negative\",\"neutral\"]\n",
    "values = [len(uniqueWords_positive),len(uniqueWords_negative),len(uniqueWords_neutral)]\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(10, 5), sharey=True)\n",
    "axs.bar(names, values)                   \n",
    "axs.set_title('Other Employee Words ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word cloud \n",
    "all_words = uniqueWords_positive + uniqueWords_negative + uniqueWords_neutral \n",
    "wordcloud = WordCloud(width = 1000, height = 500, background_color ='white').generate(' '.join(all_words))\n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are job levels significantly different in terms of network centrality (degree and betweenness centrality)? 2 t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of emails between people who both belong to job level groups\n",
    "# then subest the data by sender-recipient email total\n",
    "df_network = df[df['SenderGroup'].notnull() & df['RecipientGroup'].notnull()]\n",
    "df_weighted= df_network.groupby(['Sender','Recipient']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "G=nx.from_pandas_edgelist(df_weighted, \"Sender\", \"Recipient\", ['Message'], nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one full dictionary of the emails included in df_network\n",
    "sender_groups= dict(zip(df_network['Sender'], df_network['SenderGroup']))\n",
    "senders= list(sender_groups.keys())\n",
    "full_groups= dict(zip(df_network['Recipient'], df_network['RecipientGroup']))\n",
    "full= list(full_groups.keys())\n",
    "overlap= [s for s in senders if s not in full]\n",
    "for i in overlap:\n",
    "    full_groups[i]= sender_groups[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide full dictionary into groups based on job levels\n",
    "GroupA= [e for e in full_groups if full_groups[e]=='GroupA']\n",
    "GroupB= [e for e in full_groups if full_groups[e]=='GroupB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot network\n",
    "G=nx.from_pandas_edgelist(df_weighted, \"Sender\", \"Recipient\", ['Message'], nx.DiGraph())\n",
    "\n",
    "rednodes = GroupA\n",
    "bluenodes = GroupB\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G,pos=pos,nodelist=rednodes, node_color='red', label='Senior management', linewidths= 0.5, edgecolors='k')\n",
    "nx.draw_networkx_nodes(G,pos=pos,nodelist=bluenodes, node_color='blue', label='Other employees', linewidths= 0.5, edgecolors='k')\n",
    "nx.draw_networkx_edges(G,pos=pos)\n",
    "plt.legend(loc= 'upper right', numpoints = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of only employees in groups\n",
    "groups_df= pd.DataFrame.from_dict(full_groups, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns for degree centrality\n",
    "degCent= nx.degree_centrality(G)\n",
    "groups_df['degCent']=groups_df.index.map(degCent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for betweenness centrality\n",
    "betCent = nx.betweenness_centrality(G)\n",
    "groups_df['betCent']=groups_df.index.map(betCent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test for degree centrality between groups\n",
    "stats.ttest_ind(groups_df[groups_df[0]=='GroupA'].degCent, groups_df[groups_df[0]=='GroupB'].degCent, equal_var = True)\n",
    "# The p-value < 0.05, so the two groups are significantly different in degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test for betweenness centrality between groups\n",
    "stats.ttest_ind(groups_df[groups_df[0]=='GroupA'].betCent, groups_df[groups_df[0]=='GroupB'].betCent, equal_var = True)\n",
    "# The p-value > 0.05, so the two groups are not significantly different in betweenness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is sentiment significantly different by job levels? 1 t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subset of master df for this analysis\n",
    "sent_from_groups= df[df['SenderGroup'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test\n",
    "stats.ttest_ind(sent_from_groups[sent_from_groups['SenderGroup']=='GroupA'].Sentiment,sent_from_groups[sent_from_groups['SenderGroup']=='GroupB'].Sentiment, equal_var = False)\n",
    "# The p-value < 0.05, so the two groups are significantly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are stock price, topics, and sentiment associated? Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation coefficients between continuous measures\n",
    "corr_df= df[['Monthly Stock Price','Sentiment','Topic1','Topic2','Topic3','Topic4','Topic5']]\n",
    "corr_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all correlations are significant\n",
    "for i in corr_df.corr():\n",
    "    for j in corr_df.corr():\n",
    "        temp = df[df[i].notnull() & df[j].notnull()]\n",
    "        h= stats.spearmanr(temp[i], temp[j])\n",
    "        print(i,j,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of sentiment, sentiment by group level, topic probabilities, and monthly stock price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['Date_byMonth']=df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pivot= plot_df.pivot_table(index='Date_byMonth', values=['Sentiment','Topic1','Topic2','Topic3','Topic4','Topic5','Monthly Stock Price'], aggfunc='mean')\\\n",
    "    .dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(7,1, figsize= [20,30])\n",
    "\n",
    "ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[0].plot(new_pivot.index,new_pivot.Sentiment)\n",
    "ax[0].set_ylabel('Sentiment', fontsize= 15)\n",
    "ax[0].set_title('Mean Sentiment', fontsize=16)\n",
    "\n",
    "ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[1].plot(new_pivot.index,new_pivot['Monthly Stock Price'])\n",
    "ax[1].set_ylabel('Monthly Stock Price', fontsize= 15)\n",
    "ax[1].set_title('Monthly Stock Price', fontsize=16)\n",
    "\n",
    "ax[2].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[2].plot(new_pivot.index,new_pivot['Topic1'])\n",
    "ax[2].set_ylabel('Topic 1: Reporting', fontsize= 15)\n",
    "ax[2].set_title('Probability of Topic 1: Reporting', fontsize=16)\n",
    "\n",
    "ax[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[3].plot(new_pivot.index,new_pivot['Topic2'])\n",
    "ax[3].set_ylabel('Topic 2: Revenue', fontsize= 15)\n",
    "ax[3].set_title('Probability of Topic 2: Revenue', fontsize=16)\n",
    "\n",
    "ax[4].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[4].plot(new_pivot.index,new_pivot['Topic3'])\n",
    "ax[4].set_ylabel('Topic 3: Regulation', fontsize= 15)\n",
    "ax[4].set_title('Probability of Topic 3: Regulation', fontsize=16)\n",
    "\n",
    "ax[5].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[5].plot(new_pivot.index,new_pivot['Topic4'])\n",
    "ax[5].set_ylabel('Topic 4: Management', fontsize= 15)\n",
    "ax[5].set_title('Probability of Topic 4: Management', fontsize=16)\n",
    "\n",
    "ax[6].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax[6].plot(new_pivot.index,new_pivot['Topic5'])\n",
    "ax[6].set_ylabel('Topic 5: Energy Market', fontsize= 15)\n",
    "ax[6].set_title('Probability of Topic 5: Energy Market', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
